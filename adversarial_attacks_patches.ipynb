{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8BzHUUhZS-t"
   },
   "source": [
    "# Adversarial Patch (Target = “jellyfish”)\n",
    "\n",
    "This notebook demonstrates the process of creating an adversarial patch designed to fool a pretrained ResNet34 classifier on ImageNet. The goal is to generate a physical, printable patch that causes the model to misclassify diverse natural images as a specific target class, and in this case, “jellyfish.” The notebook walks through patch initialization, augmentation, optimization, and evaluation, following the adversarial patch framework described in Brown et al. (2017).\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ailina-aniwan/xai-adversarial-patches/blob/main/adversarial_attacks_patches.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision pillow matplotlib tqdm kornia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pY3eojEEZS-v",
    "outputId": "30a0471b-6d18-4a03-9951-9a0d6fa6f439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.2.0\n",
      "Torchvision: 0.17.0\n",
      "Device: cpu\n",
      "Target idx for 'jellyfish': 107  (jellyfish)\n"
     ]
    }
   ],
   "source": [
    "import os, glob, io, random, time\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "\n",
    "import kornia.augmentation as K\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ImageNet normalization\n",
    "IM_MEAN = torch.tensor([0.485, 0.456, 0.406], device=device).view(3,1,1)\n",
    "IM_STD  = torch.tensor([0.229, 0.224, 0.225], device=device).view(3,1,1)\n",
    "def norm(x):     # [0,1] -> normalized\n",
    "    return (x - IM_MEAN) / IM_STD\n",
    "def inv_norm(x): # normalized -> [0,1]\n",
    "    return x * IM_STD + IM_MEAN\n",
    "\n",
    "# Target index lookup for \"jellyfish\"\n",
    "weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "categories = weights.meta[\"categories\"]\n",
    "classes = categories\n",
    "target_idx = [i for i,c in enumerate(categories) if \"jellyfish\" in c.lower()][0]\n",
    "target_name = categories[target_idx]\n",
    "print(f\"Target idx for 'jellyfish': {target_idx}  ({target_name})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section imports the necessary libraries, defines the device, and sets up basic image normalization for the pretrained ResNet34 model. It also specifies the target class (\"jellyfish\") that the patch will try to induce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path & Image Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Loaded 5000 images across 10 classes: ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
      "Saved 45 clean images to sample_images/\n",
      "Saved 10 clean images to test_images/\n"
     ]
    }
   ],
   "source": [
    "# Create output folders\n",
    "os.makedirs(\"sample_images\", exist_ok=True)\n",
    "os.makedirs(\"test_images\", exist_ok=True)\n",
    "os.makedirs(\"patch_outputs\", exist_ok=True)\n",
    "\n",
    "# Load STL10 directly\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Use temporary download path\n",
    "dataset = datasets.STL10(root=\"/tmp/stl10_temp\", split=\"train\", download=True, transform=transform)\n",
    "stl10_classes = dataset.classes\n",
    "print(f\"Loaded {len(dataset)} images across {len(stl10_classes)} classes: {stl10_classes}\")\n",
    "\n",
    "# Random subset for training/testing backgrounds\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(dataset))\n",
    "train_idx = indices[:45]   # 45 training images\n",
    "test_idx  = indices[45:55] # 10 held-out test images\n",
    "\n",
    "# Save JPEGs only\n",
    "def save_images(indices, out_folder):\n",
    "    for i, idx in enumerate(indices):\n",
    "        img_tensor, label = dataset[idx]\n",
    "        img = transforms.ToPILImage()(img_tensor)\n",
    "        img = ImageOps.fit(img, (800, 800), method=Image.Resampling.LANCZOS)\n",
    "        fname = f\"{stl10_classes[label]}_{i:03d}.jpg\"\n",
    "        img.save(os.path.join(out_folder, fname), quality=95)\n",
    "    print(f\"Saved {len(indices)} clean images to {out_folder}/\")\n",
    "\n",
    "save_images(train_idx, \"sample_images\")\n",
    "save_images(test_idx, \"test_images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the STL10 dataset only as a convenient source of diverse natural images to serve as backgrounds during patch training. The class labels from STL10 are not used in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters & Print Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square patch with circular mask inside\n",
    "PATCH_SIZE = 160\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.05\n",
    "EPOCHS = 200\n",
    "TV_WEIGHT = 1e-3\n",
    "\n",
    "# Print/export settings\n",
    "PRINT_DPI = 300\n",
    "PRINT_DIAM_CM = 9\n",
    "PRINT_PX = int((PRINT_DIAM_CM / 2.54) * PRINT_DPI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define core hyperparameters such as patch size, learning rate, and total variation weight. These values control patch optimization and determine the physical dimensions for printing later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (ResNet34) & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & preprocessing ready.\n"
     ]
    }
   ],
   "source": [
    "weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "model = models.resnet34(weights=weights).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Use the canonical ImageNet normalization\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1).to(device)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1).to(device)\n",
    "\n",
    "# Preprocessing pipeline supplied by the weights\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Helper lambdas for (un)normalization\n",
    "inv_norm = lambda x: x * std + mean\n",
    "norm     = lambda x: (x - mean) / std\n",
    "\n",
    "print(\"Model & preprocessing ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the official torchvision ResNet34 and its matching normalization pipeline to avoid subtle data drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask, TV loss, and patch augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_circular_mask(h, w, radius=None, center=None):\n",
    "    yy, xx = np.mgrid[:h, :w]\n",
    "    if center is None: center = (w//2, h//2)\n",
    "    if radius is None: radius = min(h, w)//2\n",
    "    cx, cy = center\n",
    "    mask = ((xx - cx)**2 + (yy - cy)**2) <= radius**2\n",
    "    return torch.from_numpy(mask.astype(np.float32))  # [H,W]\n",
    "\n",
    "mask = make_circular_mask(PATCH_SIZE, PATCH_SIZE, radius=int(PATCH_SIZE*0.48)).unsqueeze(0).to(device)  # [1,H,W]\n",
    "\n",
    "def tv_loss(x):  # x: [B,3,H,W]\n",
    "    dh = torch.mean(torch.abs(x[:,:,1:,:] - x[:,:,:-1,:]))\n",
    "    dw = torch.mean(torch.abs(x[:,:,:,1:] - x[:,:,:,:-1]))\n",
    "    return dh + dw\n",
    "\n",
    "def augment_patch_on_image_batch(imgs_norm, patch_tensor, mask_tensor):\n",
    "    \"\"\"\n",
    "    imgs_norm: [B,3,224,224] normalized\n",
    "    patch_tensor: [3,H,W] in [0,1]\n",
    "    mask_tensor: [1,H,W]\n",
    "    returns: [B,3,224,224] normalized, with patch applied and global augs\n",
    "    \"\"\"\n",
    "    B, C, Himg, Wimg = imgs_norm.shape\n",
    "    imgs_unnorm = inv_norm(imgs_norm.clone())  # [0,1]\n",
    "\n",
    "    for i in range(B):\n",
    "        # Random scale/rotation for the patch\n",
    "        scale = random.uniform(0.6, 1.2)\n",
    "        angle = random.uniform(-30, 30)\n",
    "        pH = max(4, int(PATCH_SIZE * scale)); pW = pH\n",
    "\n",
    "        # Resize/rotate patch & mask with PIL\n",
    "        pil_patch = to_pil_image(patch_tensor.detach().cpu().clamp(0,1))\n",
    "        pil_mask  = to_pil_image(mask_tensor.squeeze(0).detach().cpu())\n",
    "        pil_patch = pil_patch.resize((pW,pH), resample=Image.BILINEAR).rotate(angle, resample=Image.BILINEAR, expand=False)\n",
    "        pil_mask  = pil_mask.resize((pW,pH), resample=Image.BILINEAR).rotate(angle, resample=Image.BILINEAR, expand=False)\n",
    "\n",
    "        pt = to_tensor(pil_patch).to(device)  # [3,pH,pW]\n",
    "        mt = to_tensor(pil_mask).to(device)   # [1,pH,pW]\n",
    "\n",
    "        # Random position\n",
    "        max_x, max_y = Wimg - pW, Himg - pH\n",
    "        x = 0 if max_x <= 0 else random.randint(0, max_x)\n",
    "        y = 0 if max_y <= 0 else random.randint(0, max_y)\n",
    "\n",
    "        # Alpha blend\n",
    "        region = imgs_unnorm[i, :, y:y+pH, x:x+pW]\n",
    "        imgs_unnorm[i, :, y:y+pH, x:x+pW] = pt * mt + region * (1 - mt)\n",
    "\n",
    "        # Occasionally simulate JPEG compression (phone camera)\n",
    "        if random.random() < 0.2:\n",
    "            pil_tmp = to_pil_image(imgs_unnorm[i].cpu().clamp(0,1))\n",
    "            q = random.randint(60, 95)\n",
    "            buf = io.BytesIO()\n",
    "            pil_tmp.save(buf, format='JPEG', quality=q)\n",
    "            buf.seek(0)\n",
    "            imgs_unnorm[i] = to_tensor(Image.open(buf).convert('RGB')).to(device)\n",
    "\n",
    "    # Apply global augs (in [0,1] pixel space), then normalize for model input\n",
    "    aug = K.AugmentationSequential(\n",
    "        K.RandomResizedCrop((224,224), scale=(0.95,1.0), ratio=(0.9,1.1), p=1.0),\n",
    "        K.RandomAffine(degrees=10, translate=0.06, scale=(0.98,1.02), p=0.6),\n",
    "        K.RandomGaussianBlur((3,3), (0.1, 2.0), p=0.25),\n",
    "        K.ColorJitter(0.25, 0.25, 0.25, 0.05, p=0.6),\n",
    "        K.RandomGrayscale(p=0.05),\n",
    "        data_keys=[\"input\"]\n",
    "    ).to(device)\n",
    "\n",
    "    imgs_aug = aug(imgs_unnorm)  # Kornia expects [0,1] range\n",
    "    imgs_back = norm(imgs_aug)   # normalize afterward\n",
    "    return imgs_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates a circular mask to constrain the patch area, defines the total variation loss to enforce smoothness, and implements the main augmentation pipeline that applies random scaling, rotation, and color perturbations to improve the patch’s robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 45\n"
     ]
    }
   ],
   "source": [
    "# Simple dataset from folder\n",
    "class SimpleImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.paths = sorted(glob.glob(os.path.join(root, \"*.*\")))\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        img = Image.open(p).convert('RGB')\n",
    "        return (self.transform(img) if self.transform else img), 0\n",
    "\n",
    "dataset = SimpleImageFolder(\"sample_images\", transform=preprocess)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0, drop_last=True)\n",
    "print(\"Train images:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple dataset class loads images from the sample_images folder and applies the same preprocessing as the pretrained model. This provides batches of backgrounds for overlaying the patch during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Patch & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize patch and optimizer\n",
    "patch = torch.rand(3, PATCH_SIZE, PATCH_SIZE, device=device, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    patch[0].mul_(0.25).add_(0.15)  # R (lower)\n",
    "    patch[1].mul_(0.25).add_(0.40)  # G (mid)\n",
    "    patch[2].mul_(0.25).add_(0.60)  # B (higher)\n",
    "    patch.clamp_(0,1)\n",
    "\n",
    "optimizer = torch.optim.Adam([patch], lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patch is initialized as a bluish tensor with slight random noise and a circular mask. The patch values are clamped to the valid [0,1] range and marked as trainable parameters for gradient-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch initialization\n",
    "PATCH_SIZE = 200\n",
    "\n",
    "base = torch.tensor([0.30, 0.50, 0.90], device=device).view(3,1,1)\n",
    "patch = (base + 0.05*torch.randn(3, PATCH_SIZE, PATCH_SIZE, device=device)).clamp(0,1)\n",
    "patch.requires_grad_(True)\n",
    "\n",
    "# circular mask (1 inside, 0 outside)\n",
    "yy, xx = torch.meshgrid(torch.arange(PATCH_SIZE, device=device),\n",
    "                        torch.arange(PATCH_SIZE, device=device),\n",
    "                        indexing='ij')\n",
    "mask = (((yy - PATCH_SIZE/2)**2 + (xx - PATCH_SIZE/2)**2) < (PATCH_SIZE/2)**2).float().unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/150 | loss=0.3077 | meanP(jellyfish)=0.99990 | grad=1.067e+01 | best=0.99990\n",
      "Epoch   5/150 | loss=-40.0440 | meanP(jellyfish)=1.00000 | grad=9.810e+00 | best=1.00000\n",
      "Epoch  10/150 | loss=-56.6253 | meanP(jellyfish)=1.00000 | grad=7.011e+00 | best=1.00000\n",
      "Epoch  15/150 | loss=-65.5386 | meanP(jellyfish)=1.00000 | grad=6.642e+00 | best=1.00000\n",
      "Epoch  20/150 | loss=-72.2801 | meanP(jellyfish)=1.00000 | grad=1.115e+01 | best=1.00000\n",
      "Epoch  25/150 | loss=-78.1701 | meanP(jellyfish)=1.00000 | grad=7.279e+00 | best=1.00000\n",
      "Epoch  30/150 | loss=-81.3215 | meanP(jellyfish)=1.00000 | grad=8.425e+00 | best=1.00000\n",
      "Epoch  35/150 | loss=-84.6282 | meanP(jellyfish)=1.00000 | grad=8.788e+00 | best=1.00000\n",
      "Epoch  40/150 | loss=-86.3415 | meanP(jellyfish)=1.00000 | grad=1.007e+01 | best=1.00000\n",
      "Epoch  45/150 | loss=-87.7628 | meanP(jellyfish)=1.00000 | grad=7.351e+00 | best=1.00000\n",
      "Epoch  50/150 | loss=-89.8029 | meanP(jellyfish)=1.00000 | grad=7.907e+00 | best=1.00000\n",
      "Epoch  55/150 | loss=-91.4546 | meanP(jellyfish)=1.00000 | grad=8.324e+00 | best=1.00000\n",
      "Epoch  60/150 | loss=-92.3664 | meanP(jellyfish)=1.00000 | grad=8.568e+00 | best=1.00000\n",
      "Epoch  65/150 | loss=-93.4966 | meanP(jellyfish)=1.00000 | grad=8.566e+00 | best=1.00000\n",
      "Epoch  70/150 | loss=-94.7954 | meanP(jellyfish)=1.00000 | grad=9.443e+00 | best=1.00000\n",
      "Epoch  75/150 | loss=-95.7933 | meanP(jellyfish)=1.00000 | grad=9.504e+00 | best=1.00000\n",
      "Epoch  80/150 | loss=-97.5768 | meanP(jellyfish)=1.00000 | grad=6.793e+00 | best=1.00000\n",
      "Epoch  85/150 | loss=-97.3724 | meanP(jellyfish)=1.00000 | grad=8.058e+00 | best=1.00000\n",
      "Epoch  90/150 | loss=-99.1050 | meanP(jellyfish)=1.00000 | grad=8.601e+00 | best=1.00000\n",
      "Epoch  95/150 | loss=-99.2816 | meanP(jellyfish)=1.00000 | grad=7.007e+00 | best=1.00000\n",
      "Epoch 100/150 | loss=-100.1304 | meanP(jellyfish)=1.00000 | grad=1.107e+01 | best=1.00000\n",
      "Epoch 105/150 | loss=-100.8377 | meanP(jellyfish)=1.00000 | grad=6.335e+00 | best=1.00000\n",
      "Epoch 110/150 | loss=-100.5831 | meanP(jellyfish)=1.00000 | grad=7.641e+00 | best=1.00000\n",
      "Epoch 115/150 | loss=-101.2280 | meanP(jellyfish)=1.00000 | grad=7.527e+00 | best=1.00000\n",
      "Epoch 120/150 | loss=-101.8732 | meanP(jellyfish)=1.00000 | grad=7.805e+00 | best=1.00000\n",
      "Epoch 125/150 | loss=-103.1615 | meanP(jellyfish)=1.00000 | grad=8.269e+00 | best=1.00000\n",
      "Epoch 130/150 | loss=-103.2503 | meanP(jellyfish)=1.00000 | grad=8.742e+00 | best=1.00000\n",
      "Epoch 135/150 | loss=-104.0472 | meanP(jellyfish)=1.00000 | grad=8.625e+00 | best=1.00000\n",
      "Epoch 140/150 | loss=-103.8240 | meanP(jellyfish)=1.00000 | grad=8.103e+00 | best=1.00000\n",
      "Epoch 145/150 | loss=-104.7260 | meanP(jellyfish)=1.00000 | grad=6.799e+00 | best=1.00000\n",
      "Epoch 150/150 | loss=-105.4554 | meanP(jellyfish)=1.00000 | grad=6.532e+00 | best=1.00000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS     = 150\n",
    "LR         = 0.02\n",
    "TV_WEIGHT  = 1e-2\n",
    "LOG_EVERY  = 5\n",
    "\n",
    "optimizer = torch.optim.Adam([patch], lr=LR)\n",
    "save_dir  = \"patch_outputs\"; os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_prob = 0.0\n",
    "loss_history, prob_history, grad_history = [], [], []\n",
    "\n",
    "def tv_loss(x):\n",
    "    dx = x[:,:,1:,:] - x[:,:,:-1,:]\n",
    "    dy = x[:,:,:,1:] - x[:,:,:,:-1]\n",
    "    return (dx.abs().mean() + dy.abs().mean())\n",
    "\n",
    "def center_overlay(img_norm_bchw, patch_01, mask_01):\n",
    "    \"\"\"img_norm_bchw is normalized; convert to [0,1], overlay, renorm.\"\"\"\n",
    "    img_pix = inv_norm(img_norm_bchw.clone())\n",
    "    pt = patch_01.clamp(0,1)\n",
    "    mt = mask_01\n",
    "\n",
    "    B, C, H, W = img_pix.shape\n",
    "    ph, pw = pt.shape[-2:]\n",
    "    y = (H - ph)//2; x = (W - pw)//2\n",
    "    img_pix[:, :, y:y+ph, x:x+pw] = pt*mt + img_pix[:, :, y:y+ph, x:x+pw]*(1-mt)\n",
    "    return norm(img_pix)  # back to normalized for the model\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    if len(loader) == 0:\n",
    "        print(\"No images in sample_images/. Add files and re-run.\")\n",
    "        break\n",
    "\n",
    "    running = 0.0\n",
    "    for imgs, _ in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        # overlay in pixel space, renormalize once\n",
    "        patched = center_overlay(imgs, patch.unsqueeze(0), mask)\n",
    "\n",
    "        logits = model(patched)                # [B, 1000]\n",
    "        tgt = logits[:, target_idx]\n",
    "        # max logit among other classes\n",
    "        logits_others = logits.clone()\n",
    "        logits_others[:, target_idx] = -1e9\n",
    "        other_max = logits_others.max(dim=1).values\n",
    "        margin = tgt - other_max\n",
    "        loss_attack = -margin.mean()           # maximize margin\n",
    "        loss_tv = TV_WEIGHT * tv_loss(patch.unsqueeze(0))\n",
    "        loss = loss_attack + loss_tv\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient diagnostics\n",
    "        grad_norm = patch.grad.detach().norm().item() if patch.grad is not None else 0.0\n",
    "\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            patch.clamp_(0,1)\n",
    "\n",
    "        running += loss.item()\n",
    "\n",
    "    avg_loss = running / len(loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    grad_history.append(grad_norm)\n",
    "\n",
    "    # quick eval on a few training imgs (center overlay)\n",
    "    if epoch % LOG_EVERY == 0 or epoch == 1:\n",
    "        eval_paths = sorted(glob.glob(\"sample_images/*.*\"))[:12]\n",
    "        probs = []\n",
    "        for p in eval_paths:\n",
    "            x = preprocess(Image.open(p).convert(\"RGB\")).to(device).unsqueeze(0)\n",
    "            x_patched = center_overlay(x, patch.unsqueeze(0), mask)  # reuse helper\n",
    "            with torch.no_grad():\n",
    "                pr = F.softmax(model(x_patched), dim=1)[0, target_idx].item()\n",
    "            probs.append(pr)\n",
    "\n",
    "        mean_prob = float(sum(probs)/len(probs)) if probs else 0.0\n",
    "        prob_history.append((epoch, mean_prob))\n",
    "\n",
    "        # save snapshots\n",
    "        to_pil_image(patch.detach().cpu()).save(os.path.join(save_dir, f\"patch_epoch_{epoch:03d}.png\"))\n",
    "        if mean_prob > best_prob:\n",
    "            best_prob = mean_prob\n",
    "            torch.save(patch.detach().cpu(), os.path.join(save_dir, \"best_patch.pt\"))\n",
    "            to_pil_image(patch.detach().cpu()).save(os.path.join(save_dir, \"best_patch.png\"))\n",
    "\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS} | loss={avg_loss:.4f} | \"\n",
    "              f\"meanP(jellyfish)={mean_prob:.5f} | grad={grad_norm:.3e} | best={best_prob:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop optimizes the patch to increase the ResNet34 model’s confidence in the “jellyfish” class. The loss combines a margin-based adversarial objective with total variation regularization for smoothness. The patch is clamped after each update and periodically saved for inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Print-Ready Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Export complete.\n",
      "Saved single large patch: patch_outputs/jellyfish_patch_print_600px.png\n",
      "Saved 3x3 sticker sheet: patch_outputs/jellyfish_patch_sticker_sheet_3x3.png\n"
     ]
    }
   ],
   "source": [
    "PRINT_PX = 600  # adjust for print size\n",
    "\n",
    "# Load the best saved patch\n",
    "best_png = os.path.join(\"patch_outputs\", \"best_patch.png\")\n",
    "if os.path.exists(best_png):\n",
    "    best_patch_pil = Image.open(best_png).convert(\"RGB\")\n",
    "else:\n",
    "    best_patch_pil = to_pil_image(patch.detach().cpu().clamp(0, 1))\n",
    "    best_patch_pil.save(os.path.join(\"patch_outputs\", \"final_patch.png\"))\n",
    "\n",
    "# Create single large print patch\n",
    "final_print = best_patch_pil.resize((PRINT_PX, PRINT_PX), resample=Image.LANCZOS)\n",
    "single_path = os.path.join(\"patch_outputs\", f\"jellyfish_patch_print_{PRINT_PX}px.png\")\n",
    "final_print.save(single_path, dpi=(300, 300))\n",
    "\n",
    "# Create 3x3 sticker sheet layout\n",
    "gap = 40  # pixel spacing between stickers\n",
    "sheet_w = PRINT_PX * 3 + gap * 4\n",
    "sheet_h = PRINT_PX * 3 + gap * 4\n",
    "canvas = Image.new(\"RGB\", (sheet_w, sheet_h), (255, 255, 255))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        x = gap + j * (PRINT_PX + gap)\n",
    "        y = gap + i * (PRINT_PX + gap)\n",
    "        canvas.paste(final_print, (x, y))\n",
    "\n",
    "sheet_path = os.path.join(\"patch_outputs\", \"jellyfish_patch_sticker_sheet_3x3.png\")\n",
    "canvas.save(sheet_path, dpi=(300, 300))\n",
    "\n",
    "print(\"\\nExport complete.\")\n",
    "print(\"Saved single large patch:\", single_path)\n",
    "print(\"Saved 3x3 sticker sheet:\", sheet_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the best-performing patch is resized for physical printing. Both a single large patch and a 3×3 sticker sheet are created to test the attack under real-world conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Held-Out Photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird_005.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "bird_007.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "car_008.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "cat_002.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "deer_004.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "dog_001.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "horse_000.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "horse_003.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "monkey_006.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "monkey_009.jpg -> pred: jellyfish prob(jellyfish)=1.000\n",
      "\n",
      "Top-1 success rate: 1.0\n",
      "Mean target probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on held-out test_images/\n",
    "test_paths = sorted(glob.glob(\"test_images/*.*\"))\n",
    "if not test_paths:\n",
    "    print(\"Add photos to test_images/ for evaluation.\")\n",
    "else:\n",
    "    successes, probs = 0, []\n",
    "    for pth in test_paths:\n",
    "        img = preprocess(Image.open(pth).convert(\"RGB\")).to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            img_u = inv_norm(img.clone())\n",
    "            pt = to_tensor(best_patch_pil).to(device)\n",
    "            mt = mask.to(device)\n",
    "            _,_,H,W = img_u.shape\n",
    "            x = (W - pt.shape[2])//2; y = (H - pt.shape[1])//2\n",
    "            # overlay patch at image center\n",
    "            img_u[0,:, y:y+pt.shape[1], x:x+pt.shape[2]] = pt*mt + img_u[0,:, y:y+pt.shape[1], x:x+pt.shape[2]]*(1-mt)\n",
    "            \n",
    "            # forward through model\n",
    "            out = model(norm(img_u))\n",
    "            prob = F.softmax(out, dim=1)[0, target_idx].item()\n",
    "            pred = torch.argmax(out, dim=1).item()\n",
    "\n",
    "        # print results (use ImageNet class list)\n",
    "        print(os.path.basename(pth), \"-> pred:\", categories[pred], f\"prob({target_name})={prob:.3f}\")\n",
    "        probs.append(prob)\n",
    "        if pred == target_idx: \n",
    "            successes += 1\n",
    "\n",
    "    print(\"\\nTop-1 success rate:\", successes / len(test_paths))\n",
    "    print(\"Mean target probability:\", sum(probs) / len(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the final patch on unseen test images by overlaying it at the image center and measuring the model’s predicted probabilities for the target class. The output includes predicted labels, target probabilities, and overall success rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TktaoexsZS-3"
   },
   "source": [
    "## References\n",
    "\n",
    "This tutorial was originally created by Phillip Lippe. \n",
    "[![View notebooks on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb)  \n",
    "\n",
    "[1] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" ICLR 2015.\n",
    "\n",
    "[2] Hendrik Metzen, Jan, et al. \"Universal adversarial perturbations against semantic image segmentation.\" Proceedings of the IEEE International Conference on Computer Vision. 2017.\n",
    "\n",
    "[3] Anant Jain. \"Breaking neural networks with adversarial attacks.\" [Blog post](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa) 2019."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "056d3d1e6f6b454882315f865dd7f3f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08e490ac36b8469ca75cb2d2c3b28733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af569713b3b8418b80c5c3ee7f7dc74b",
      "max": 157,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c6b1a2240524b15ae30852fcef7f0af",
      "value": 157
     }
    },
    "0978e7d29c8d40069864468d44c945fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21cfe00ec0a1455e90dfe09733df3a2d",
      "placeholder": "​",
      "style": "IPY_MODEL_ec3ccd3fc07c40f78dc897dde53cb2ca",
      "value": " 157/157 [16:38&lt;00:00,  4.81s/it]"
     }
    },
    "16cea9b9b8cf421bb9654d89322a0cd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1da9430bcbe048478813c2762ddf0a2b",
      "placeholder": "​",
      "style": "IPY_MODEL_355903401fe2485ba32b4f9ce0ed6e52",
      "value": "Validating...:   0%"
     }
    },
    "1add540e466345e6b9ecb1d82c084511": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eb1aa4da369b4d5e8b6b86c12d053ca2",
       "IPY_MODEL_08e490ac36b8469ca75cb2d2c3b28733",
       "IPY_MODEL_0978e7d29c8d40069864468d44c945fc"
      ],
      "layout": "IPY_MODEL_b059d79a2bef4c8aabc65980dd38707d"
     }
    },
    "1c73eeeae9b240ba889e8250cb4cc69b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91bbd38185da48d08030ce05655e19ad",
      "placeholder": "​",
      "style": "IPY_MODEL_d2f1a42fcbed4f01afc75866066e2bda",
      "value": " 0/157 [00:22&lt;?, ?it/s]"
     }
    },
    "1da9430bcbe048478813c2762ddf0a2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21cfe00ec0a1455e90dfe09733df3a2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "355903401fe2485ba32b4f9ce0ed6e52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "687d2db8d16044aa9d397840d92b8e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf516300a7fa4db5b7dfd9a35f48f302",
      "max": 157,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9900392f804e46ce96e4ece2ae69100d",
      "value": 0
     }
    },
    "8c6b1a2240524b15ae30852fcef7f0af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "91bbd38185da48d08030ce05655e19ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9900392f804e46ce96e4ece2ae69100d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af569713b3b8418b80c5c3ee7f7dc74b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b059d79a2bef4c8aabc65980dd38707d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf516300a7fa4db5b7dfd9a35f48f302": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf6bccac8e144adba07c701e0918dcf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16cea9b9b8cf421bb9654d89322a0cd2",
       "IPY_MODEL_687d2db8d16044aa9d397840d92b8e35",
       "IPY_MODEL_1c73eeeae9b240ba889e8250cb4cc69b"
      ],
      "layout": "IPY_MODEL_dff47b3829b74120aab697182e5524c6"
     }
    },
    "d2f1a42fcbed4f01afc75866066e2bda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dff47b3829b74120aab697182e5524c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb1aa4da369b4d5e8b6b86c12d053ca2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_056d3d1e6f6b454882315f865dd7f3f3",
      "placeholder": "​",
      "style": "IPY_MODEL_fb02301952554aa88c62b9ab683c2e9a",
      "value": "Validating...: 100%"
     }
    },
    "ec3ccd3fc07c40f78dc897dde53cb2ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb02301952554aa88c62b9ab683c2e9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
